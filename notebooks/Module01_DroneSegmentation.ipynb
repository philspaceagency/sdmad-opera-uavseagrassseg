{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbbc2fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rasterio\n",
      "  Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting affine (from rasterio)\n",
      "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.4.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2025.10.5)\n",
      "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.3.0)\n",
      "Collecting cligj>=0.5 (from rasterio)\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.0.2)\n",
      "Collecting click-plugins (from rasterio)\n",
      "  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio) (3.2.5)\n",
      "Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
      "Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
      "Successfully installed affine-2.4.0 click-plugins-1.1.1.2 cligj-0.7.2 rasterio-1.4.3\n",
      "Requirement already satisfied: geopandas in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from geopandas) (2.0.2)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from geopandas) (0.11.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from geopandas) (25.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (2.2.2)\n",
      "Requirement already satisfied: pyproj>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (3.7.2)\n",
      "Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2025.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from pyogrio>=0.7.2->geopandas) (2025.10.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->geopandas) (1.17.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: shapely in /usr/local/lib/python3.12/dist-packages (2.1.2)\n",
      "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from shapely) (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install rasterio\n",
    "!pip install geopandas\n",
    "!pip install tqdm\n",
    "!pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1e4877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from tifffile import imread\n",
    "from rasterio.windows import Window\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f2d2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_orthomosaic_tiff(tiff_path, max_size=2048):\n",
    "    \"\"\"\n",
    "    Load orthomosaic TIFF file with proper handling\n",
    "    \"\"\"\n",
    "    print(f\"Loading orthomosaic: {tiff_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Method 1: Using rasterio (better for geoTIFF)\n",
    "        with rasterio.open(tiff_path) as src:\n",
    "            image = src.read()\n",
    "            image = np.transpose(image, (1, 2, 0))  # (H, W, C)\n",
    "            print(f\"Loaded with rasterio - Shape: {image.shape}, dtype: {image.dtype}\")\n",
    "    except:\n",
    "        # Method 2: Using tifffile\n",
    "        try:\n",
    "            image = imread(tiff_path)\n",
    "            if len(image.shape) == 3 and image.shape[0] in [3, 4]:\n",
    "                image = np.transpose(image, (1, 2, 0))  # (C, H, W) -> (H, W, C)\n",
    "            print(f\"Loaded with tifffile - Shape: {image.shape}, dtype: {image.dtype}\")\n",
    "        except:\n",
    "            # Method 3: Using PIL\n",
    "            image = np.array(Image.open(tiff_path))\n",
    "            print(f\"Loaded with PIL - Shape: {image.shape}, dtype: {image.dtype}\")\n",
    "    \n",
    "    # Handle different channel configurations\n",
    "    if image.shape[-1] > 3:\n",
    "        print(f\"Keeping first 3 channels from {image.shape[-1]} channels\")\n",
    "        image = image[:, :, :3]\n",
    "    \n",
    "    # Convert to uint8 if needed\n",
    "    if image.dtype == np.uint16:\n",
    "        image = (image / 256).astype(np.uint8)\n",
    "    elif image.dtype == np.float32 or image.dtype == np.float64:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "    \n",
    "    # Resize if too large (for memory efficiency)\n",
    "    h, w = image.shape[:2]\n",
    "    if max(h, w) > max_size:\n",
    "        scale = max_size / max(h, w)\n",
    "        new_h, new_w = int(h * scale), int(w * scale)\n",
    "        image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "        print(f\"Resized to: {image.shape}\")\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d37aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DINOv2 Feature Extractor\n",
    "# ============================================================================\n",
    "\n",
    "class DINOv2FeatureExtractor:\n",
    "    \"\"\"Extract features using DINOv2 foundation model\"\"\"\n",
    "    def __init__(self, model_size='small', device='cuda'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_size: 'small', 'base', 'large', or 'giant'\n",
    "            device: 'cuda' or 'cpu'\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.model_size = model_size\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Loading DINOv2 ({model_size}) model...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Load DINOv2 model\n",
    "        model_map = {\n",
    "            'small': 'dinov2_vits14',\n",
    "            'base': 'dinov2_vitb14',\n",
    "            'large': 'dinov2_vitl14',\n",
    "            'giant': 'dinov2_vitg14'\n",
    "        }\n",
    "        \n",
    "        self.model = torch.hub.load('facebookresearch/dinov2', model_map[model_size])\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Get feature dimension\n",
    "        self.feature_dim = self.model.embed_dim\n",
    "        \n",
    "        print(f\"✓ Model loaded: {model_map[model_size]}\")\n",
    "        print(f\"  Feature dimension: {self.feature_dim}\")\n",
    "        print(f\"  Patch size: 14x14\")\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"Preprocess image for DINOv2\"\"\"\n",
    "        # Convert to float and normalize\n",
    "        if image.dtype == np.uint8:\n",
    "            image = image.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Handle grayscale\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.stack([image] * 3, axis=-1)\n",
    "        \n",
    "        # ImageNet normalization\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = (image - mean) / std\n",
    "        \n",
    "        # Convert to tensor (C, H, W)\n",
    "        image_tensor = torch.FloatTensor(image).permute(2, 0, 1)\n",
    "        \n",
    "        return image_tensor\n",
    "    \n",
    "    def extract_features(self, image, stride=14):\n",
    "        \"\"\"\n",
    "        Extract dense features from image\n",
    "        \n",
    "        Args:\n",
    "            image: Input image (H, W, C) or (H, W)\n",
    "            stride: Stride for feature extraction (default: 14, same as patch size)\n",
    "        \n",
    "        Returns:\n",
    "            features: Dense feature map (H', W', feature_dim)\n",
    "            feature_spatial_size: (H', W')\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Extracting DINOv2 Features\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        print(f\"Image size: {h} x {w}\")\n",
    "        \n",
    "        # Preprocess\n",
    "        image_tensor = self.preprocess_image(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get patch features\n",
    "            features = self.model.forward_features(image_tensor)\n",
    "            \n",
    "            # Extract patch tokens (remove CLS token)\n",
    "            patch_features = features['x_norm_patchtokens']  # (1, N_patches, feature_dim)\n",
    "            \n",
    "            # Calculate spatial dimensions\n",
    "            patch_h = h // 14\n",
    "            patch_w = w // 14\n",
    "            \n",
    "            # Reshape to spatial grid\n",
    "            patch_features = patch_features.squeeze(0)  # (N_patches, feature_dim)\n",
    "            patch_features = patch_features.reshape(patch_h, patch_w, self.feature_dim)\n",
    "            \n",
    "            print(f\"✓ Features extracted\")\n",
    "            print(f\"  Feature map size: {patch_h} x {patch_w} x {self.feature_dim}\")\n",
    "            print(f\"  Reduction factor: {14}x\")\n",
    "        \n",
    "        return patch_features.cpu().numpy(), (patch_h, patch_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98252d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Spectral Clustering on DINOv2 Features\n",
    "# ============================================================================\n",
    "\n",
    "def spectral_clustering_segmentation(features, n_clusters=5, \n",
    "                                    sigma=0.5, normalize=True):\n",
    "    \"\"\"\n",
    "    Perform spectral clustering on DINOv2 features for segmentation\n",
    "    \n",
    "    Args:\n",
    "        features: Feature map (H, W, feature_dim)\n",
    "        n_clusters: Number of segments\n",
    "        sigma: Gaussian kernel bandwidth\n",
    "        normalize: Normalize features before clustering\n",
    "    \n",
    "    Returns:\n",
    "        segmentation_map: Segmentation labels (H, W)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Spectral Clustering (n_clusters={n_clusters})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    h, w, d = features.shape\n",
    "    \n",
    "    # Reshape features\n",
    "    features_flat = features.reshape(-1, d)\n",
    "    \n",
    "    # Normalize features\n",
    "    if normalize:\n",
    "        features_flat = features_flat / (np.linalg.norm(features_flat, axis=1, keepdims=True) + 1e-8)\n",
    "    \n",
    "    print(f\"Computing affinity matrix...\")\n",
    "    \n",
    "    # Compute affinity matrix using spatial and feature similarity\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    from sklearn.cluster import SpectralClustering\n",
    "    \n",
    "    # For large images, use approximate spectral clustering\n",
    "    n_samples = features_flat.shape[0]\n",
    "    \n",
    "    if n_samples > 10000:\n",
    "        print(f\"  Large image ({n_samples} pixels), using approximate clustering...\")\n",
    "        # Subsample for affinity computation\n",
    "        indices = np.random.choice(n_samples, size=min(5000, n_samples), replace=False)\n",
    "        features_sample = features_flat[indices]\n",
    "        \n",
    "        # Compute affinity on subsample\n",
    "        affinity_sample = np.exp(-euclidean_distances(features_sample, features_sample) / (2 * sigma**2))\n",
    "        \n",
    "        # Cluster subsample\n",
    "        sc = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', \n",
    "                               n_init=10, assign_labels='kmeans')\n",
    "        labels_sample = sc.fit_predict(affinity_sample)\n",
    "        \n",
    "        # Assign all pixels to nearest cluster center\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors=1)\n",
    "        knn.fit(features_sample, labels_sample)\n",
    "        labels = knn.predict(features_flat)\n",
    "    else:\n",
    "        # Full spectral clustering for smaller images\n",
    "        sc = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors',\n",
    "                               n_neighbors=10, n_init=10, assign_labels='kmeans')\n",
    "        labels = sc.fit_predict(features_flat)\n",
    "    \n",
    "    # Reshape to image\n",
    "    segmentation_map = labels.reshape(h, w)\n",
    "    \n",
    "    print(f\"✓ Clustering complete\")\n",
    "    for label in np.unique(labels):\n",
    "        count = np.sum(labels == label)\n",
    "        print(f\"  Segment {label}: {count} pixels ({count/len(labels)*100:.1f}%)\")\n",
    "    \n",
    "    return segmentation_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# K-Nearest Neighbors on DINOv2 Features\n",
    "# ============================================================================\n",
    "\n",
    "def knn_segmentation(features, n_clusters=5, normalize=True):\n",
    "    \"\"\"\n",
    "    Simple K-Means clustering on DINOv2 features\n",
    "    \n",
    "    Args:\n",
    "        features: Feature map (H, W, feature_dim)\n",
    "        n_clusters: Number of segments\n",
    "        normalize: Normalize features before clustering\n",
    "    \n",
    "    Returns:\n",
    "        segmentation_map: Segmentation labels (H, W)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"K-Means Clustering (n_clusters={n_clusters})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    h, w, d = features.shape\n",
    "    \n",
    "    # Reshape features\n",
    "    features_flat = features.reshape(-1, d)\n",
    "    \n",
    "    # Normalize features\n",
    "    if normalize:\n",
    "        features_flat = features_flat / (np.linalg.norm(features_flat, axis=1, keepdims=True) + 1e-8)\n",
    "    \n",
    "    from sklearn.cluster import MiniBatchKMeans\n",
    "    \n",
    "    print(f\"Running K-Means...\")\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, \n",
    "                            batch_size=1000, n_init=10)\n",
    "    labels = kmeans.fit_predict(features_flat)\n",
    "    \n",
    "    # Reshape to image\n",
    "    segmentation_map = labels.reshape(h, w)\n",
    "    \n",
    "    print(f\"✓ Clustering complete\")\n",
    "    for label in np.unique(labels):\n",
    "        count = np.sum(labels == label)\n",
    "        print(f\"  Segment {label}: {count} pixels ({count/len(labels)*100:.1f}%)\")\n",
    "    \n",
    "    return segmentation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "724addd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Post-processing: Upsampling and Refinement\n",
    "# ============================================================================\n",
    "\n",
    "def upsample_segmentation(segmentation_map, original_size, method='nearest'):\n",
    "    \"\"\"\n",
    "    Upsample segmentation map to original image size\n",
    "    \n",
    "    Args:\n",
    "        segmentation_map: Low-res segmentation (H', W')\n",
    "        original_size: Target size (H, W)\n",
    "        method: 'nearest', 'bilinear', or 'guided'\n",
    "    \n",
    "    Returns:\n",
    "        upsampled_map: Full resolution segmentation (H, W)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Upsampling Segmentation\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    h_target, w_target = original_size\n",
    "    h_low, w_low = segmentation_map.shape\n",
    "    \n",
    "    print(f\"Upsampling: {h_low}x{w_low} -> {h_target}x{w_target}\")\n",
    "    \n",
    "    if method == 'nearest':\n",
    "        # Simple nearest neighbor upsampling\n",
    "        upsampled = cv2.resize(segmentation_map, (w_target, h_target), \n",
    "                              interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    elif method == 'bilinear':\n",
    "        # Bilinear then round\n",
    "        upsampled = cv2.resize(segmentation_map.astype(np.float32), \n",
    "                              (w_target, h_target), \n",
    "                              interpolation=cv2.INTER_LINEAR)\n",
    "        upsampled = np.round(upsampled).astype(np.int32)\n",
    "    \n",
    "    print(f\"✓ Upsampling complete\")\n",
    "    \n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def refine_segmentation(segmentation_map, image, iterations=3):\n",
    "    \"\"\"\n",
    "    Refine segmentation using guided filtering\n",
    "    \n",
    "    Args:\n",
    "        segmentation_map: Segmentation labels\n",
    "        image: Original image for guidance\n",
    "        iterations: Number of refinement iterations\n",
    "    \n",
    "    Returns:\n",
    "        refined_map: Refined segmentation\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Refining Segmentation ({iterations} iterations)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Convert to grayscale if needed\n",
    "    if len(image.shape) == 3:\n",
    "        guide = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        guide = (image * 255).astype(np.uint8)\n",
    "    \n",
    "    refined = segmentation_map.copy()\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Apply bilateral filter to each label separately\n",
    "        refined_float = refined.astype(np.float32)\n",
    "        refined_float = cv2.bilateralFilter(refined_float, d=9, \n",
    "                                           sigmaColor=75, sigmaSpace=75)\n",
    "        refined = np.round(refined_float).astype(np.int32)\n",
    "    \n",
    "    print(f\"✓ Refinement complete\")\n",
    "    \n",
    "    return refined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8025eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_segmentation(image, segmentation_map, save_path=None):\n",
    "    \"\"\"Visualize segmentation results with DINOv2\"\"\"\n",
    "    \n",
    "    n_clusters = len(np.unique(segmentation_map))\n",
    "    \n",
    "    # Create colormap\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, n_clusters))\n",
    "    cmap = ListedColormap(colors)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Original image\n",
    "    if len(image.shape) == 2:\n",
    "        axes[0].imshow(image, cmap='gray')\n",
    "    else:\n",
    "        if image.dtype == np.float32 or image.dtype == np.float64:\n",
    "            axes[0].imshow(np.clip(image, 0, 1))\n",
    "        else:\n",
    "            axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Segmentation map\n",
    "    im = axes[1].imshow(segmentation_map, cmap=cmap, interpolation='nearest')\n",
    "    axes[1].set_title(f'DINOv2 Segmentation ({n_clusters} segments)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Overlay\n",
    "    if len(image.shape) == 2:\n",
    "        overlay = np.stack([image]*3, axis=-1)\n",
    "    else:\n",
    "        overlay = image.copy()\n",
    "    \n",
    "    if overlay.dtype != np.float32 and overlay.dtype != np.float64:\n",
    "        overlay = overlay.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Create colored overlay\n",
    "    seg_colored = cmap(segmentation_map / n_clusters)[:, :, :3]\n",
    "    overlay_blend = 0.6 * overlay + 0.4 * seg_colored\n",
    "    \n",
    "    axes[2].imshow(np.clip(overlay_blend, 0, 1))\n",
    "    axes[2].set_title('Overlay', fontsize=14, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"✓ Visualization saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_features(features, method='pca', save_path=None):\n",
    "    \"\"\"Visualize DINOv2 features using dimensionality reduction\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Visualizing Features ({method.upper()})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    h, w, d = features.shape\n",
    "    features_flat = features.reshape(-1, d)\n",
    "    \n",
    "    if method == 'pca':\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=3)\n",
    "        features_reduced = pca.fit_transform(features_flat)\n",
    "        print(f\"  Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "    elif method == 'tsne':\n",
    "        from sklearn.manifold import TSNE\n",
    "        tsne = TSNE(n_components=3, random_state=42)\n",
    "        features_reduced = tsne.fit_transform(features_flat)\n",
    "    \n",
    "    # Reshape and normalize to [0, 1]\n",
    "    features_rgb = features_reduced.reshape(h, w, 3)\n",
    "    features_rgb = (features_rgb - features_rgb.min()) / (features_rgb.max() - features_rgb.min())\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(features_rgb)\n",
    "    plt.title(f'DINOv2 Features ({method.upper()} visualization)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Feature visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ec5778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Complete Pipeline with DINOv2\n",
    "# ============================================================================\n",
    "\n",
    "def dinov2_segmentation_pipeline(image, n_clusters=5, \n",
    "                                model_size='small',\n",
    "                                refine=True,\n",
    "                                device='cuda'):\n",
    "    \"\"\"\n",
    "    Complete unsupervised segmentation pipeline using DINOv2\n",
    "    \n",
    "    Args:\n",
    "        image: Input drone image (H, W, C) or (H, W)\n",
    "        n_clusters: Number of segments\n",
    "        model_size: 'small', 'base', 'large', or 'giant'\n",
    "        clustering_method: 'kmeans', 'spectral', or 'hierarchical'\n",
    "        refine: Apply post-processing refinement\n",
    "        device: 'cuda' or 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        segmentation_map: Pixel-wise segmentation labels\n",
    "        features: DINOv2 features\n",
    "        extractor: DINOv2 feature extractor\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DINOv2 UNSUPERVISED SEGMENTATION PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Clustering method: {clustering_method.upper()}\")\n",
    "    print(f\"Number of clusters: {n_clusters}\")\n",
    "    \n",
    "    # Ensure image is in correct format\n",
    "    if image.dtype == np.uint8:\n",
    "        image_display = image.astype(np.float32) / 255.0\n",
    "    else:\n",
    "        image_display = image.copy()\n",
    "    \n",
    "    # Step 1: Extract DINOv2 features\n",
    "    print(\"\\n[1/4] Extracting DINOv2 features...\")\n",
    "    extractor = DINOv2FeatureExtractor(model_size=model_size, device=device)\n",
    "    features, feature_size = extractor.extract_features(image)\n",
    "    \n",
    "    # Step 2: Cluster features\n",
    "    print(f\"\\n[2/4] Clustering features...\")\n",
    "    segmentation_low = knn_segmentation(features, n_clusters=n_clusters)\n",
    "\n",
    "    \n",
    "    # Step 3: Upsample to original resolution\n",
    "    print(f\"\\n[3/4] Upsampling to original resolution...\")\n",
    "    segmentation_map = upsample_segmentation(segmentation_low, image.shape[:2])\n",
    "    \n",
    "    # Step 4: Optional refinement\n",
    "    if refine:\n",
    "        print(f\"\\n[4/4] Refining segmentation...\")\n",
    "        segmentation_map = refine_segmentation(segmentation_map, image_display)\n",
    "    else:\n",
    "        print(f\"\\n[4/4] Skipping refinement\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return segmentation_map, features, extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef51f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"DINOv2-based Unsupervised Drone Image Segmentation\")\n",
    "    print(\"\\nThis pipeline:\")\n",
    "    print(\"1. Extracts dense DINOv2 features (no training needed!)\")\n",
    "    print(\"2. Clusters features using spatial-aware methods\")\n",
    "    print(\"3. Produces high-quality segmentation\")\n",
    "    print(\"\\nAdvantages over traditional methods:\")\n",
    "    print(\"✓ No training required - uses pre-trained DINOv2\")\n",
    "    print(\"✓ Better semantic understanding\")\n",
    "    print(\"✓ Robust to lighting/viewpoint changes\")\n",
    "    print(\"✓ Works on diverse drone imagery\")\n",
    "    print(\"\\nExample usage:\")\n",
    "\n",
    "    import cv2\n",
    "    \n",
    "    # Load drone image\n",
    "    tiff_path = \n",
    "    image = load_orthomosaic_tiff(tiff_path, max_size=512)\n",
    "    \n",
    "    # Run DINOv2 segmentation\n",
    "    seg_map, features, extractor = dinov2_segmentation_pipeline(\n",
    "        image=image,\n",
    "        n_clusters=6,\n",
    "        model_size='small',  # 'small', 'base', 'large', 'giant'\n",
    "        clustering_method='hierarchical',  # 'kmeans', 'spectral', 'hierarchical'\n",
    "        refine=True\n",
    "    )\n",
    "    \n",
    "    # Visualize\n",
    "    visualize_segmentation(image, seg_map)\n",
    "    visualize_features(features, method='pca')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
